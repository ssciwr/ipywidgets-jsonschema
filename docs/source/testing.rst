Testing
=======
This section outlines the testing setup and how to extend or contribute to the test suite.

Test Suite Overview
-------------------
The project utilizes the ``pytest`` framework for testing. All test cases are located in the ``tests/`` directory at the root
of the repository.

Running Tests
-------------
To execute the test suite:

Make sure you have ``pytest`` installed.

.. code-block:: bash

    python -m pip install -U pytest

**Run tests**: Navigate to the project's directory and run:

.. code-block:: bash

    pytest

This command will discover and execute all tests in the ``tests/`` directory.




Testing setup
-------------
Tests are automatically created using:

- ``.json`` schema files from the `tests/schemas` directory
- ``pydantic`` models from the `tests/models` directory


The test framework uses a custom ``conftest.py`` file that loads all schemas and models, and passes them into test functions via ``pytest_generate_tests``.

How it works
------------
In ``conftest.py``, the hook ``pytest_generate_tests(metafunc)`` inspects the arguments of each test function.
If a test function uses either ``testcase`` or ``model`` as a parameter, pytest will inject multiple values into that function using ``@pytest.mark.parametrize``.

.. code-block:: python

    def pytest_generate_tests(metafunc):
        if "testcase" in metafunc.fixturenames:
            metafunc.parametrize("testcase", _test_data, ids=_test_names)

        if "model" in metafunc.fixturenames:
            metafunc.parametrize("model", models, ids=[model.__name__ for model in models])

Each test is labeled using its corresponding filename or class name via the `ids=...` argument, which helps keep the test output readable.


Fixtures provided
-----------------
- ``testcase``: a schema and test cases with ``default``, ``valid`` and ``invalid`` documents
- ``model``: ``pydantic`` model class with ``valid_cases()`` and ``invalid_cases()`` methods.





All tests live in ``test_form.py``. There are two main types:

Schema-based tests
------------------

These use schemas defined in ``schemas/*.json``. Each schema can define:

- A **default** document to compare with generated form data
- A list of **valid** documents that must pass validation
- A list of **invalid** documents that must raise an error

Example:

.. code-block:: json

    {
        "default": "foo",
        "schema": {
            "default": "foo",
            "description": "Bla bla",
            "type": "string"
        },
        "valid": [
            "Bla"
        ]
    }


Model-based tests
------------------
Pydantic models define their own schema and test cases by implementing:

.. code-block:: python

    @classmethod
    def valid_cases(cls):
        """Provide valid cases for the model."""
        return []

    @classmethod
    def invalid_cases(cls):
        """Provide invalid cases for the model."""
        return []

    @classmethod
    def default_values(cls):
        """Provide default values for the model."""
        return {}

These are used to verify that:

- The `JSON schema` generated by that model works correctly in the form.
- Default values to compare with generated form data
- A list of **valid** documents that must pass validation
- A list of **invalid** documents that must raise an error


Summary
-------
To add a new test case:
- Create a new .json file in ``schemas/``
- Or define a new model in ``models/`` and add ``.valid_cases()`` and ``.invalid_cases()``

There’s no need to write custom test functions manually. Just add schemas and models, and they’ll be picked up automatically.
